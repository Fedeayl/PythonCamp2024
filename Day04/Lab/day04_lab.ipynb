{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Web Scraping + File I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions: \n",
    "\n",
    "1. Go to https://polisci.wustl.edu/people/88/all OR https://polisci.wustl.edu/people/list/88/all\n",
    "2. Go to the page for each of the professors.\n",
    "3. Create a `.csv`` file with the following information for each professor:\n",
    "\t- Name\n",
    "\t- Title\n",
    "\t- E-mail\n",
    "\t- Web page\n",
    "\t- Specialization  \n",
    "\t\t- If they do not have a specialization, you can leave it blank. \n",
    "\t\t- An example from Deniz's page: https://polisci.wustl.edu/people/deniz-aksoy\n",
    "\t\t- Professor Aksoyâ€™s research is motivated by an interest in comparative political institutions and political violence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Hint: one solution involves importing the following as well\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "\n",
    "driver_path = Service('/Users/Fede/Desktop/PhD - Courses/Python/PythonCamp2024/Day04/Lecture/chromedriver')\n",
    "driver = webdriver.Chrome(service = driver_path)\n",
    "\n",
    "url = \"https://polisci.wustl.edu/people/88/all\"\n",
    "\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "soup = bs(driver.page_source, 'html.parser')\n",
    "\n",
    "professor_links = [a['href'] for a in soup.find_all('a', href=True) if '/people/' in a['href']]\n",
    "\n",
    "professors_data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/people/deniz-aksoy', '/people/timm-betz', '/people/zachary-bowersox', '/people/christina-l-boyd', '/people/daniel-butler', '/people/taylor-carlson', '/people/david-carter', '/people/dino-p-christenson', '/people/brian-f-crisp', '/people/alfred-darnell', '/people/ted-enamorado', '/people/lee-epstein', '/people/justin-fox', '/people/matthew-gabel', '/people/amy-gais', '/people/james-l-gibson', '/people/matthew-hayes', '/people/clarissa-rile-hayward', '/people/jaclyn-kaslovsky', '/people/frank-lovett', '/people/christopher-lucas', '/people/andrew-martin', '/people/jacob-montgomery', '/people/lucia-motolinia']\n"
     ]
    }
   ],
   "source": [
    "print(professor_links[3:len(professor_links)-1])\n",
    "\n",
    "professor_links = professor_links[3:len(professor_links)-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "zac = \"https://polisci.wustl.edu/people/zachary-bowersox\"\n",
    "\n",
    "driver.get(zac)\n",
    "time.sleep(5)\n",
    "\n",
    "zacprof = bs(driver.page_source, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to professors.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import csv\n",
    "import time\n",
    "\n",
    "\n",
    "driver_path = Service('/Users/Fede/Desktop/PhD - Courses/Python/PythonCamp2024/Day04/Lecture/chromedriver')\n",
    "driver = webdriver.Chrome(service = driver_path)\n",
    "\n",
    "\n",
    "# Configure Selenium\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "driver_path = Service('/Users/Fede/Desktop/PhD - Courses/Python/PythonCamp2024/Day04/Lecture/chromedriver')\n",
    "driver = webdriver.Chrome(service = driver_path, options=chrome_options)\n",
    "\n",
    "# URL of the faculty list page\n",
    "url = 'https://polisci.wustl.edu/people/88/all'\n",
    "driver.get(url)\n",
    "\n",
    "# Give the page some time to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Use BeautifulSoup to parse the page source\n",
    "soup = bs(driver.page_source, 'html.parser')\n",
    "\n",
    "# Scroll and load all professors\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    # Scroll down to the bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Wait for new profiles to load\n",
    "    time.sleep(3)  # Adjust this depending on the loading time of the page\n",
    "\n",
    "    # Get new height after scroll\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    # Break the loop if no new profiles were loaded\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    \n",
    "    last_height = new_height\n",
    "\n",
    "# Use BeautifulSoup to parse the page source after all profiles are loaded\n",
    "soup = bs(driver.page_source, 'html.parser')\n",
    "\n",
    "# Find all professor profile links\n",
    "professor_links = [a['href'] for a in soup.find_all('a', href=True) if '/people/' in a['href']]\n",
    "professor_links = professor_links[3:len(professor_links)-1]\n",
    "\n",
    "\n",
    "# CSV file to save the information\n",
    "csv_file = 'professors.csv'\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Name', 'Title', 'Email', 'Web page', 'Specialization'])\n",
    "\n",
    "    # Loop through each professor link\n",
    "    for link in professor_links:\n",
    "        driver.get(f\"https://polisci.wustl.edu{link}\")\n",
    "        time.sleep(2)  # Let the page load\n",
    "\n",
    "        prof_soup = bs(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Extract the required information with checks for NoneType\n",
    "        meta_name_element = prof_soup.find('meta', property='og:title')\n",
    "        name = meta_name_element['content'].strip() if meta_name_element else 'N/A'\n",
    "\n",
    "        title_element = prof_soup.find('div', class_='title')\n",
    "        title = title_element.text.strip() if title_element else 'N/A'\n",
    "\n",
    "        email_element = prof_soup.find('a', href=lambda href: href and \"mailto:\" in href)\n",
    "        email = email_element.text.strip() if email_element else 'N/A'\n",
    "\n",
    "        webpage = f\"https://polisci.wustl.edu{link}\"  # The link you are visiting is the webpage\n",
    "\n",
    "        specialization_element = prof_soup.find('div', class_='heading', string='research interests:')\n",
    "        specialization = ''\n",
    "        if specialization_element:\n",
    "            interests_list = specialization_element.find_next('ul', class_='interests')\n",
    "            if interests_list:\n",
    "                specialization = ', '.join([li.text.strip() for li in interests_list.find_all('li')])\n",
    "\n",
    "        # Write the data to the CSV file\n",
    "        writer.writerow([name, title, email, webpage, specialization])\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "print(f'Data has been saved to {csv_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/people/deniz-aksoy', '/people/timm-betz', '/people/zachary-bowersox', '/people/christina-l-boyd', '/people/daniel-butler', '/people/taylor-carlson', '/people/david-carter', '/people/dino-p-christenson', '/people/brian-f-crisp', '/people/alfred-darnell', '/people/ted-enamorado', '/people/lee-epstein', '/people/justin-fox', '/people/matthew-gabel', '/people/amy-gais', '/people/james-l-gibson', '/people/matthew-hayes', '/people/clarissa-rile-hayward', '/people/jaclyn-kaslovsky', '/people/frank-lovett', '/people/christopher-lucas', '/people/andrew-martin', '/people/jacob-montgomery', '/people/lucia-motolinia']\n"
     ]
    }
   ],
   "source": [
    "print(professor_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
